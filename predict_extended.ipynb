{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596778915619",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt, log2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss, ndcg_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "import json\n",
    "from fancyimpute import KNN\n",
    "from IPython.display import display_html\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_users_2.csv')\n",
    "df_test = pd.read_csv('test_users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -unknown- frequency in gender coloumn is nearly as half as much of the total number of samples.\n",
    "# Removing or replacing this value with mean/media or imputing using knn will definitely generate noise. Therefore, it's best to leave it as is.\n",
    "# Handling missing values for test sets is usually problematic especially for features that are both imprtant and have large numbers of missing values.\n",
    "# I will concatenate train and test sets to fill missing values using knn and then seperate them back.\n",
    "# This approach will ensure reasonable accuracy.\n",
    "\n",
    "df_train.drop(['id'], axis=1, inplace=True)\n",
    "df_test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "df_train.drop(['date_first_booking'], axis=1, inplace=True)\n",
    "df_test.drop(['date_first_booking'], axis=1, inplace=True)\n",
    "\n",
    "df_train['age'] = df_train['age'].apply(lambda x: np.nan if ((x < 18) or (x > 117)) else x)\n",
    "df_test['age'] = df_test['age'].apply(lambda x: np.nan if ((x < 18) or (x > 117)) else x)\n",
    "\n",
    "#df_train['age_bucket'] = pd.cut(df_train['age'], [18, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 120], right=False)\n",
    "#df_test['age_bucket'] = pd.cut(df_test['age'], [18, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 120], right=False)\n",
    "\n",
    "#df_train['gender'].replace('-unknown-', np.nan, inplace = True)\n",
    "#df_test['gender'].replace('-unknown-', np.nan, inplace = True)\n",
    "\n",
    "# Extracting different time features from 'date_account_created' and 'timestamp_first_active'\n",
    "seasons = {1: 'winter', 2:'winter', 12:'winter', 3:'spring', 4:'spring', 5:'spring', 6:'summer', 7:'summer', 8:'summer', 9:'autumn', 10:'autumn', 11:'autumn'}\n",
    "\n",
    "# Train Set\n",
    "df_train['date_account_created'] = pd.to_datetime(df_train['date_account_created'])\n",
    "df_train['year_ac'] = pd.DatetimeIndex(df_train['date_account_created']).year\n",
    "df_train['month_ac'] = pd.DatetimeIndex(df_train['date_account_created']).month\n",
    "df_train['day_ac'] = pd.DatetimeIndex(df_train['date_account_created']).day\n",
    "df_train['weekday_ac'] = pd.DatetimeIndex(df_train['date_account_created']).weekday\n",
    "df_train['season_ac'] = df_train['month_ac'].apply(lambda x: seasons[x])\n",
    "\n",
    "df_train['time_first_active'] = pd.to_datetime((df_train['timestamp_first_active']), format='%Y%m%d%H%M%S')\n",
    "df_train['year_fa'] = pd.DatetimeIndex(df_train['time_first_active']).year\n",
    "df_train['month_fa'] = pd.DatetimeIndex(df_train['time_first_active']).month\n",
    "df_train['day_fa'] = pd.DatetimeIndex(df_train['time_first_active']).day\n",
    "df_train['weekday_fa'] = pd.DatetimeIndex(df_train['time_first_active']).weekday\n",
    "df_train['season_fa'] = df_train['month_fa'].apply(lambda x: seasons[x])\n",
    "\n",
    "df_train.drop(['date_account_created'], axis=1, inplace=True)\n",
    "df_train.drop(['timestamp_first_active'], axis=1, inplace=True)\n",
    "df_train.drop(['time_first_active'], axis=1, inplace=True)\n",
    "\n",
    "# Test Set\n",
    "df_test['date_account_created'] = pd.to_datetime(df_test['date_account_created'])\n",
    "df_test['year_ac'] = pd.DatetimeIndex(df_test['date_account_created']).year\n",
    "df_test['month_ac'] = pd.DatetimeIndex(df_test['date_account_created']).month\n",
    "df_test['day_ac'] = pd.DatetimeIndex(df_test['date_account_created']).day\n",
    "df_test['weekday_ac'] = pd.DatetimeIndex(df_test['date_account_created']).weekday\n",
    "df_test['season_ac'] = df_test['month_ac'].apply(lambda x: seasons[x])\n",
    "\n",
    "df_test['time_first_active'] = pd.to_datetime((df_test['timestamp_first_active']), format='%Y%m%d%H%M%S')\n",
    "df_test['year_fa'] = pd.DatetimeIndex(df_test['time_first_active']).year\n",
    "df_test['month_fa'] = pd.DatetimeIndex(df_test['time_first_active']).month\n",
    "df_test['day_fa'] = pd.DatetimeIndex(df_test['time_first_active']).day\n",
    "df_test['weekday_fa'] = pd.DatetimeIndex(df_test['time_first_active']).weekday\n",
    "df_test['season_fa'] = df_test['month_fa'].apply(lambda x: seasons[x])\n",
    "\n",
    "df_test.drop(['date_account_created'], axis=1, inplace=True)\n",
    "df_test.drop(['timestamp_first_active'], axis=1, inplace=True)\n",
    "df_test.drop(['time_first_active'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.concat([df_train,df_test], keys=[0,1])\n",
    "\n",
    "# Frequency dictionaries\n",
    "frq_suf = df_temp['signup_flow'].value_counts().to_dict()\n",
    "frq_lang = df_temp['language'].value_counts().to_dict()\n",
    "frq_ap = df_temp['affiliate_provider'].value_counts().to_dict()\n",
    "frq_fdt = df_temp['first_device_type'].value_counts().to_dict()\n",
    "frq_brws = df_temp['first_browser'].value_counts().to_dict()\n",
    "\n",
    "threshold = 37 #group values below this threshold\n",
    "\n",
    "df_temp['signup_flow'] = df_train['signup_flow'].apply(lambda x: 'other' if frq_suf[x] < threshold else x)\n",
    "df_temp['language'] = df_train['language'].apply(lambda x: 'other' if frq_lang[x] < threshold else x)\n",
    "df_temp['affiliate_provider'] = df_train['affiliate_provider'].apply(lambda x: 'other' if frq_ap[x] < threshold else x)\n",
    "df_temp['first_device_type'].replace(['Mac Desktop', 'Windows Desktop', 'Desktop (Other)'], 'Desktop', inplace = True)\n",
    "df_temp['first_device_type'].replace(['iPhone', 'Android Phone', 'SmartPhone (Other)'], 'Smartphone', inplace = True)\n",
    "df_temp['first_device_type'].replace(['iPad', 'Android Tablet'], 'Tablet', inplace = True)\n",
    "df_temp['first_browser'] = df_train['first_browser'].apply(lambda x: 'other' if frq_brws[x] < threshold else x)\n",
    "\n",
    "# Previously I extracted seasons from month_ac and month_fa to use for prediction and data visualisation.\n",
    "# I realise now that this may create problems when predicting due to the fact that month and season features are highly correlated.\n",
    "# Will be kept above for visualisation purposes (later).\n",
    "df_temp.drop(['season_ac', 'season_fa'], axis=1, inplace=True)\n",
    "\n",
    "# Since we now have day of the week, I'll drop the day coloumns\n",
    "#df_temp.drop(['day_ac', 'day_fa'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I will combine train and test sets into a temporary dataframe to fill missing values using knn.\n",
    "# The temp dataframe will be seperated back into train and test sets after imputation.\n",
    "# first_affiliate_tracked coloumn has null values that need to be imputed.\n",
    "# country_destination will have new null values after combining datasets.\n",
    "# Non-null values of first_affiliate_tracked and country_destination will be encoded seperately and inserted back into df_temp using numpy squeeze.\n",
    "\n",
    "#df_temp = pd.concat([df_train,df_test], keys=[0,1])\n",
    "\n",
    "categories = ['gender', 'signup_method', 'signup_flow', 'language',  'affiliate_channel', 'affiliate_provider', 'signup_app', 'first_device_type', 'first_browser']\n",
    "le = LabelEncoder()\n",
    "le_f = LabelEncoder()\n",
    "\n",
    "first_affiliate_nonulls = np.array(df_temp['first_affiliate_tracked'].dropna())\n",
    "first_affiliate_nonulls = first_affiliate_nonulls.reshape(-1,1)\n",
    "first_affiliate_encoded = le_f.fit_transform(first_affiliate_nonulls)\n",
    "df_temp['first_affiliate_tracked'].loc[df_temp['first_affiliate_tracked'].notnull()] = np.squeeze(first_affiliate_encoded)\n",
    "\n",
    "country_dest_nonulls = np.array(df_temp['country_destination'].dropna())\n",
    "country_dest_nonulls = country_dest_nonulls.reshape(-1,1)\n",
    "country_dest_encoded = le.fit_transform(country_dest_nonulls)\n",
    "df_temp['country_destination'].loc[df_temp['country_destination'].notnull()] = np.squeeze(country_dest_encoded)\n",
    "\n",
    "for i in categories:\n",
    "    df_temp[i] = le_f.fit_transform(df_temp[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Imputation (This will take ~20 minutes)\n",
    "# You can skip this step. I've already run it and exported the filled df to df_impute.csv (included in main directory)\n",
    "\n",
    "def get_k(n, chunks=10):\n",
    "    k = round(sqrt(n/chunks))\n",
    "    if k % 2 == 0:\n",
    "        return k-1\n",
    "    else:\n",
    "        return k\n",
    "        \n",
    "chunks = 10\n",
    "k = get_k(n=df_temp.shape[0], chunks=chunks)\n",
    "df_temp_2 = df_temp.copy()\n",
    "df_temp_2.drop(['country_destination'], axis=1, inplace=True)\n",
    "cols = list(df_temp_2.columns)\n",
    "df_impute = pd.DataFrame()\n",
    "\n",
    "for chunk in np.array_split(df_temp_2, chunks):\n",
    "    chunk = np.round(KNN(k=k).fit_transform(chunk))\n",
    "    df_chunk = pd.DataFrame(data=chunk, columns=cols)\n",
    "    df_impute = pd.concat([df_impute, df_chunk], axis=0, ignore_index=True)\n",
    "\n",
    "df_impute.to_csv('df_impute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell if you skipped the previous one.\n",
    "df_impute = pd.read_csv('df_impute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell is just for making sure nothing is messed up before imputing into df_temp\n",
    "# Check for null values\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display_html(html_str.replace('table', 'table style=\"display:inline\"'), raw=True)\n",
    "\n",
    "x = 0\n",
    "for i in df_impute.columns:\n",
    "    sum = df_impute[i].isnull().sum()\n",
    "    if sum != 0:\n",
    "        print(i + ' has: {}'.format(sum) + ' NaN')\n",
    "        x += 1\n",
    "if x == 0:\n",
    "    print('No null values in df_impute.\\n')\n",
    "\n",
    "# First, insert imputed values from df_impute into a new temp df to ensure order\n",
    "age_imputed = df_impute['age'].to_numpy()\n",
    "fa_imputed = df_impute['first_affiliate_tracked'].to_numpy()\n",
    "\n",
    "df_temp_4 = df_temp.copy()\n",
    "df_temp_4['age'] = age_imputed\n",
    "df_temp_4['first_affiliate_tracked'] = fa_imputed\n",
    "\n",
    "print('Original shape\\tFilled shape')\n",
    "print(''+str(df_temp.shape)+'\\t'+str(df_temp_4.shape))\n",
    "\n",
    "print('\\nOriginal\\t\\tFilled')\n",
    "display_side_by_side(df_temp[['age', 'first_affiliate_tracked']].head(10), df_temp_4[['age', 'first_affiliate_tracked']].head(10))\n",
    "\n",
    "#splitting df_temp\n",
    "df_train_2, df_test_2 = df_temp.xs(0), df_temp.xs(1)\n",
    "df_test_2.drop(['country_destination'], axis=1, inplace=True)\n",
    "\n",
    "#splitting df_temp_4\n",
    "df_train_3, df_test_3 = df_temp_4.xs(0), df_temp_4.xs(1)\n",
    "df_test_3.drop(['country_destination'], axis=1, inplace=True)\n",
    "\n",
    "print('\\nOriginal train\\t\\tFilled train')\n",
    "display_side_by_side(df_train_2[['age', 'first_affiliate_tracked']].head(10), df_train_3[['age', 'first_affiliate_tracked']].head(10))\n",
    "print('\\nOriginal test\\t\\tFilled test')\n",
    "display_side_by_side(df_test_2[['age', 'first_affiliate_tracked']].head(10), df_test_3[['age', 'first_affiliate_tracked']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we can split the data back safely, we can apply this on our data\n",
    "\n",
    "age_imputed = df_impute['age'].to_numpy()\n",
    "fa_imputed = df_impute['first_affiliate_tracked'].to_numpy()\n",
    "\n",
    "df_temp['age'] = age_imputed\n",
    "df_temp['first_affiliate_tracked'] = fa_imputed\n",
    "\n",
    "df_target = df_temp.xs(0)['country_destination']\n",
    "target = df_target.values\n",
    "\n",
    "df_temp.drop(['country_destination'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encoding Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = ['gender', 'signup_method', 'signup_flow', 'language',  'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "ct = ColumnTransformer([('encoder', OneHotEncoder(), categories)], remainder='passthrough')\n",
    "matrix = ct.fit_transform(df_temp)\n",
    "df = pd.DataFrame(matrix)\n",
    "train = df[:len(df_train)]\n",
    "test = df[len(df_train):]\n",
    "\n",
    "train = train.values\n",
    "test = test.values\n",
    "df_target = df_target.astype(int)\n",
    "target = df_target.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.33, random_state=28, shuffle=True)\n",
    "\n",
    "pd.DataFrame(X_train).to_csv('X_train.csv')\n",
    "pd.DataFrame(X_test).to_csv('X_test.csv')\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv')\n",
    "pd.DataFrame(y_test).to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a multi-class classification problem. The prediction output for each row is the top 5 country destinations\n",
    "# with highest probability. Using sklearn accuracy metric won't give real accuracy.\n",
    "# NDCG (Normalized Discounted Cumulative Gain) is the evaluation metric used for this competition.\n",
    "\n",
    "def acc_ndcg(y_test, y_pred_proba):\n",
    "    acc = []\n",
    "    for i in range(len(y_pred_proba)):\n",
    "        p = np.argsort(y_pred_proba[i])[-5:][::-1]\n",
    "        t = y_test[i]\n",
    "        try:\n",
    "            index = int(np.where(p == t)[0][0]+1)\n",
    "            dcg = 1/(log2(index + 1))\n",
    "        except:\n",
    "            dcg = 0.0\n",
    "        acc.append(dcg)\n",
    "    return np.mean(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# CatBoostClassifier max_depth grid search (This will take a very long time. Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = catboost.CatBoostClassifier(loss_function='MultiClass', task_type='GPU')\n",
    "max_depth = range(7, 16, 1)\n",
    "param_grid = dict(max_depth=max_depth)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\")\n",
    "grid_result = grid_search.fit(train, target)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(max_depth, means, yerr=stds)\n",
    "plt.title(\"CatBoost max_depth vs Log Loss\")\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('catboost_max_depth_2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# CatBoostClassifier learning_rate grid search (This will take a very long time. Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = catboost.CatBoostClassifier(loss_function='MultiClass', task_type='GPU', max_depth=11)\n",
    "learning_rate = np.arange(0.01, 0.08, 0.01)\n",
    "param_grid = dict(learning_rate=learning_rate)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\")\n",
    "grid_result = grid_search.fit(train, target)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(learning_rate, means, yerr=stds)\n",
    "plt.title(\"CatBoost learning_rate vs Log Loss\")\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('catboost_learning_rate2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# CatBoostClassifier Training\n",
    "\n",
    "clf_cb = CatBoostClassifier(loss_function='MultiClass', task_type='GPU', max_depth=11, iterations=1000, learning_rate=0.01, random_state=28, logging_level='Verbose')\n",
    "clf_cb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = clf_cb.predict_proba(X_test)\n",
    "y_pred = clf_cb.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of CatBoost: %s'%(acc))\n",
    "lloss = log_loss(y_test, y_pred_proba)\n",
    "print('Log Loss of Catboost: %s'%(lloss))\n",
    "ndcg = acc_ndcg(y_test, y_pred_proba)\n",
    "print('NDCG of CatBoost: %s'%(ndcg))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix of CatBoost:\\n%s'%(cm))\n",
    "\n",
    "clf_cb.save_model(str(type(clf_cb).__name__)+'.json', format='json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBClassifier n_estimators grid search (This will take a long time. Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = XGBClassifier()\n",
    "n_estimators = range(10, 35, 5)\n",
    "param_grid = dict(n_estimators=n_estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(n_estimators, means, yerr=stds)\n",
    "plt.title(\"XGBoost n_estimators vs Log Loss\")\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('n_estimators.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBClassifier learning_rate grid search (This will take a long time. Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = XGBClassifier(n_estimators=25)\n",
    "learning_rate = np.arange(0.2, 0.5, 0.05)\n",
    "param_grid = dict(learning_rate=learning_rate)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(learning_rate, means, yerr=stds)\n",
    "plt.title(\"XGBoost learning_rate vs Log Loss\")\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('learning_rate.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBClassifier max_depth grid search (This will take a long time. Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = XGBClassifier(n_estimators=25, learning_rate=0.3)\n",
    "max_depth = range(3, 10, 1)\n",
    "param_grid = dict(max_depth=max_depth)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(max_depth, means, yerr=stds)\n",
    "plt.title(\"XGBoost max_depth vs Log Loss\")\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('max_depth.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBClassifier Training\n",
    "\n",
    "clf_xgb = XGBClassifier(max_depth=5, learning_rate=0.3, n_estimators=25, objective='multi:softprob', colsample_bytree=0.6, seed=28, verbosity=1)\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "y_pred= clf_xgb.predict(X_test)\n",
    "y_pred_proba = clf_xgb.predict_proba(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of XGB: %s'%(acc))\n",
    "lloss = log_loss(y_test, y_pred_proba)\n",
    "print('Log Loss of XGB: %s'%(lloss))\n",
    "ndcg = acc_ndcg(y_test, y_pred_proba)\n",
    "print('NDCG of XGB: %s'%(ndcg))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix of XGB:\\n%s'%(cm))\n",
    "\n",
    "clf_xgb.save_model(str(type(clf_xgb).__name__)+'.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier max_depth grid search (Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "max_depth = range(2, 10, 1)\n",
    "param_grid = dict(max_depth=max_depth)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(max_depth, means, yerr=stds)\n",
    "plt.title(\"DTree max_depth vs Log Loss\")\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('dtree_max_depth.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier Training\n",
    "\n",
    "clf_dtree = DecisionTreeClassifier(max_depth=4)\n",
    "clf_dtree.fit(X_train, y_train)\n",
    "y_pred= clf.predict(X_test)\n",
    "y_pred_proba = clf_dtree.predict_proba(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of DTree: %s'%(acc))\n",
    "ndcg = acc_ndcg(y_test, y_pred_proba)\n",
    "print('NDCG of DTree: %s'%(ndcg))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix of DTree:\\n%s'%(cm))\n",
    "\n",
    "joblib.dump(clf_dtree, str(type(clf_dtree).__name__)+'.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SVM Training\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.33, random_state=28, shuffle=True)\n",
    "\n",
    "clf_svm = SVC(decision_function_shape='ovo', probability=True, random_state=28)\n",
    "clf_svm.fit(X_train, y_train)\n",
    "y_pred= clf_svm.predict(X_test)\n",
    "y_pred_proba = clf_svm.predict_proba(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of SVM: %s'%(acc))\n",
    "ndcg = acc_ndcg(y_test, y_pred_proba)\n",
    "print('NDCG of SVM: %s'%(ndcg))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix of SVM:\\n%s'%(cm))\n",
    "\n",
    "joblib.dump(clf_svm, 'SVM.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier max_depth grid search (Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "max_depth = range(2, 16, 1)\n",
    "param_grid = dict(max_depth=max_depth)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(max_depth, means, yerr=stds)\n",
    "plt.title(\"RandomForrest max_depth vs Log Loss\")\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('RandomForrest_max_depth.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier n_estimators grid search (Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "n_estimators = range(50, 300, 50)\n",
    "param_grid = dict(n_estimators=n_estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(n_estimators, means, yerr=stds)\n",
    "plt.title(\"RandomForrest n_estimators vs Log Loss\")\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('RandomForrest_n_estimators.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier min_samples_split grid search (Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=250, criterion='entropy', max_depth=11)\n",
    "min_samples_split = range(50, 350, 25)\n",
    "param_grid = dict(min_samples_split=min_samples_split)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(min_samples_split, means, yerr=stds)\n",
    "plt.title(\"RandomForrest min_samples_split vs Log Loss\")\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('RandomForrest_min_samples_split.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier min_samples_leaf grid search (Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=250, criterion='entropy', max_depth=11, min_samples_split=50)\n",
    "min_samples_leaf = range(50, 350, 25)\n",
    "param_grid = dict(min_samples_leaf=min_samples_leaf)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(min_samples_leaf, means, yerr=stds)\n",
    "plt.title(\"RandomForrest min_samples_leaf vs Log Loss\")\n",
    "plt.xlabel('min_samples_leaf')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('RandomForrest_min_samples_leaf.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier Training \n",
    "\n",
    "clf_rndforrest = RandomForestClassifier(n_estimators=250, criterion='entropy', max_depth=11, min_samples_split=50, min_samples_leaf=50)\n",
    "\n",
    "clf_rndforrest.fit(X_train, y_train)\n",
    "y_pred = clf_rndforrest.predict(X_test)\n",
    "y_pred_proba = clf_rndforrest.predict_proba(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of RandomForrest: %s'%(acc))\n",
    "ndcg = acc_ndcg(y_test, y_pred_proba)\n",
    "print('ndcg of RandomForrest: %s'%(ndcg))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix of RandomForrest:\\n%s'%(cm))\n",
    "\n",
    "joblib.dump(clf_rndforrest, str(type(clf_rndforrest).__name__)+'.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KNeighborsClassifier k grid search (Results are included in pdf)\n",
    "# The code for the grid search is taken from this webpage: https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/\n",
    "\n",
    "def get_k(n):\n",
    "    k = round(sqrt(n))\n",
    "    if k % 2 == 0:\n",
    "        return k-1\n",
    "    else:\n",
    "        return k\n",
    "\n",
    "model = KNeighborsClassifier(weights='distance')\n",
    "n_neighbors = range(5, get_k(len(train))+20, 10)\n",
    "param_grid = dict(n_neighbors=n_neighbors)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=28)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "plt.errorbar(n_neighbors, means, yerr=stds)\n",
    "plt.title(\"KNN n_neighbors vs Log Loss\")\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.savefig('KNN_n_neighbors.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KNeighborsClassifier Training\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=475, weights='distance')\n",
    "\n",
    "clf_knn.fit(X_train, y_train)\n",
    "y_pred = clf_knn.predict(X_test)\n",
    "y_pred_proba = clf_knn.predict_proba(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of KNN: %s'%(acc))\n",
    "ndcg = acc_ndcg(y_test, y_pred_proba)\n",
    "print('ndcg of KNN: %s'%(ndcg))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix of KNN:\\n%s'%(cm))\n",
    "\n",
    "joblib.dump(clf_knn, str(type(clf_knn).__name__)+'.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predicting\n",
    "\n",
    "y_pred_proba = clf_cb.predict_proba(test)\n",
    "submission = pd.DataFrame(columns=['id', 'country_destination'])\n",
    "temp = pd.read_csv('test_users.csv')\n",
    "temp = temp['id'].values\n",
    "idx = []\n",
    "cd = []\n",
    "for i in temp:\n",
    "    for _ in range(5):\n",
    "        idx.append(i)\n",
    "idx = np.array(idx)\n",
    "submission['id'] = idx\n",
    "\n",
    "for i in range(len(y_pred_proba)):\n",
    "    pred = le.inverse_transform(np.argsort(y_pred_proba[i])[-5:][::-1])\n",
    "    for p in pred:\n",
    "        cd.append(p)\n",
    "cd = np.array(cd)\n",
    "submission['country_destination'] = cd\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  }
 ]
}