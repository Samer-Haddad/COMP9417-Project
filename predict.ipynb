{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596847583941",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt, log2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import json\n",
    "import joblib\n",
    "from fancyimpute import KNN\n",
    "from IPython.display import display_html\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_users_2.csv')\n",
    "df_test = pd.read_csv('test_users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -unknown- frequency in gender coloumn is nearly as half as much of the total number of samples.\n",
    "# Removing or replacing this value with mean/media or imputing using knn will generate noise. Therefore, it's best to leave it as is.\n",
    "# Handling missing values for test sets is usually problematic especially for features that are both imprtant and have large numbers of missing values.\n",
    "# Therefore, I will concatenate train and test sets to fill missing values using knn and then seperate them back.\n",
    "# This approach will ensure reasonable accuracy.\n",
    "\n",
    "df_train.drop(['id'], axis=1, inplace=True)\n",
    "df_test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "df_train.drop(['date_first_booking'], axis=1, inplace=True)\n",
    "df_test.drop(['date_first_booking'], axis=1, inplace=True)\n",
    "\n",
    "df_train['age'] = df_train['age'].apply(lambda x: np.nan if ((x < 18) or (x > 117)) else x)\n",
    "df_test['age'] = df_test['age'].apply(lambda x: np.nan if ((x < 18) or (x > 117)) else x)\n",
    "\n",
    "# Extracting different time features from 'date_account_created' and 'timestamp_first_active'\n",
    "seasons = {1: 'winter', 2:'winter', 12:'winter', 3:'spring', 4:'spring', 5:'spring', 6:'summer', 7:'summer', 8:'summer', 9:'autumn', 10:'autumn', 11:'autumn'}\n",
    "\n",
    "# Train Set\n",
    "df_train['date_account_created'] = pd.to_datetime(df_train['date_account_created'])\n",
    "df_train['year_ac'] = pd.DatetimeIndex(df_train['date_account_created']).year\n",
    "df_train['month_ac'] = pd.DatetimeIndex(df_train['date_account_created']).month\n",
    "df_train['day_ac'] = pd.DatetimeIndex(df_train['date_account_created']).day\n",
    "df_train['weekday_ac'] = pd.DatetimeIndex(df_train['date_account_created']).weekday\n",
    "df_train['season_ac'] = df_train['month_ac'].apply(lambda x: seasons[x])\n",
    "\n",
    "df_train['time_first_active'] = pd.to_datetime((df_train['timestamp_first_active']), format='%Y%m%d%H%M%S')\n",
    "df_train['year_fa'] = pd.DatetimeIndex(df_train['time_first_active']).year\n",
    "df_train['month_fa'] = pd.DatetimeIndex(df_train['time_first_active']).month\n",
    "df_train['day_fa'] = pd.DatetimeIndex(df_train['time_first_active']).day\n",
    "df_train['weekday_fa'] = pd.DatetimeIndex(df_train['time_first_active']).weekday\n",
    "df_train['season_fa'] = df_train['month_fa'].apply(lambda x: seasons[x])\n",
    "\n",
    "df_train.drop(['date_account_created'], axis=1, inplace=True)\n",
    "df_train.drop(['timestamp_first_active'], axis=1, inplace=True)\n",
    "df_train.drop(['time_first_active'], axis=1, inplace=True)\n",
    "\n",
    "# Test Set\n",
    "df_test['date_account_created'] = pd.to_datetime(df_test['date_account_created'])\n",
    "df_test['year_ac'] = pd.DatetimeIndex(df_test['date_account_created']).year\n",
    "df_test['month_ac'] = pd.DatetimeIndex(df_test['date_account_created']).month\n",
    "df_test['day_ac'] = pd.DatetimeIndex(df_test['date_account_created']).day\n",
    "df_test['weekday_ac'] = pd.DatetimeIndex(df_test['date_account_created']).weekday\n",
    "df_test['season_ac'] = df_test['month_ac'].apply(lambda x: seasons[x])\n",
    "\n",
    "df_test['time_first_active'] = pd.to_datetime((df_test['timestamp_first_active']), format='%Y%m%d%H%M%S')\n",
    "df_test['year_fa'] = pd.DatetimeIndex(df_test['time_first_active']).year\n",
    "df_test['month_fa'] = pd.DatetimeIndex(df_test['time_first_active']).month\n",
    "df_test['day_fa'] = pd.DatetimeIndex(df_test['time_first_active']).day\n",
    "df_test['weekday_fa'] = pd.DatetimeIndex(df_test['time_first_active']).weekday\n",
    "df_test['season_fa'] = df_test['month_fa'].apply(lambda x: seasons[x])\n",
    "\n",
    "df_test.drop(['date_account_created'], axis=1, inplace=True)\n",
    "df_test.drop(['timestamp_first_active'], axis=1, inplace=True)\n",
    "df_test.drop(['time_first_active'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I will combine train and test sets into a temporary dataframe to fill missing values using knn.\n",
    "# The temp dataframe will be seperated back into train and test sets after imputation.\n",
    "# This approach reduced from 163 to 43 dimensions.\n",
    "\n",
    "df_temp = pd.concat([df_train,df_test], keys=[0,1])\n",
    "\n",
    "# Frequency dictionaries\n",
    "frq_suf = df_temp['signup_flow'].value_counts().to_dict()\n",
    "frq_lang = df_temp['language'].value_counts().to_dict()\n",
    "frq_ap = df_temp['affiliate_provider'].value_counts().to_dict()\n",
    "frq_fdt = df_temp['first_device_type'].value_counts().to_dict()\n",
    "frq_brws = df_temp['first_browser'].value_counts().to_dict()\n",
    "\n",
    "threshold = 37 # group values with frequency below this threshold into one feature\n",
    "\n",
    "df_temp['signup_flow'] = df_train['signup_flow'].apply(lambda x: 'other' if frq_suf[x] < threshold else x)\n",
    "df_temp['language'] = df_train['language'].apply(lambda x: 'other' if frq_lang[x] < threshold else x)\n",
    "df_temp['affiliate_provider'] = df_train['affiliate_provider'].apply(lambda x: 'other' if frq_ap[x] < threshold else x)\n",
    "df_temp['first_device_type'].replace(['Mac Desktop', 'Windows Desktop', 'Desktop (Other)'], 'Desktop', inplace = True)\n",
    "df_temp['first_device_type'].replace(['iPhone', 'Android Phone', 'SmartPhone (Other)'], 'Smartphone', inplace = True)\n",
    "df_temp['first_device_type'].replace(['iPad', 'Android Tablet'], 'Tablet', inplace = True)\n",
    "df_temp['first_browser'] = df_train['first_browser'].apply(lambda x: 'other' if frq_brws[x] < threshold else x)\n",
    "\n",
    "# Previously I extracted seasons from month_ac and month_fa to use for prediction and data visualisation.\n",
    "# I realise now that this may create problems when predicting due to the fact that month and season features are highly correlated.\n",
    "# Will be kept above for visualisation purposes (later).\n",
    "df_temp.drop(['season_ac', 'season_fa'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_affiliate_tracked coloumn has null values that need to be imputed.\n",
    "# country_destination will have new null values after combining datasets.\n",
    "# Non-null values of first_affiliate_tracked and country_destination will be encoded seperately and inserted back into df_temp using numpy squeeze.\n",
    "\n",
    "categories = ['gender', 'signup_method', 'signup_flow', 'language',  'affiliate_channel', 'affiliate_provider', 'signup_app', 'first_device_type', 'first_browser']\n",
    "le = LabelEncoder()\n",
    "le_f = LabelEncoder()\n",
    "\n",
    "first_affiliate_nonulls = np.array(df_temp['first_affiliate_tracked'].dropna())\n",
    "first_affiliate_nonulls = first_affiliate_nonulls.reshape(-1,1)\n",
    "first_affiliate_encoded = le_f.fit_transform(first_affiliate_nonulls)\n",
    "df_temp['first_affiliate_tracked'].loc[df_temp['first_affiliate_tracked'].notnull()] = np.squeeze(first_affiliate_encoded)\n",
    "\n",
    "country_dest_nonulls = np.array(df_temp['country_destination'].dropna())\n",
    "country_dest_nonulls = country_dest_nonulls.reshape(-1,1)\n",
    "country_dest_encoded = le.fit_transform(country_dest_nonulls)\n",
    "df_temp['country_destination'].loc[df_temp['country_destination'].notnull()] = np.squeeze(country_dest_encoded)\n",
    "\n",
    "for i in categories:\n",
    "    df_temp[i] = le_f.fit_transform(df_temp[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Imputation (This will take ~20 minutes)\n",
    "# You can skip this step. I've already run it and exported the filled df to df_impute.csv (included in main directory)\n",
    "\n",
    "def get_k(n, chunks=10):\n",
    "    k = round(sqrt(n/chunks))\n",
    "    if k % 2 == 0:\n",
    "        return k-1\n",
    "    else:\n",
    "        return k\n",
    "        \n",
    "chunks = 10\n",
    "k = get_k(n=df_temp.shape[0], chunks=chunks)\n",
    "df_temp_2 = df_temp.copy()\n",
    "df_temp_2.drop(['country_destination'], axis=1, inplace=True)\n",
    "cols = list(df_temp_2.columns)\n",
    "df_impute = pd.DataFrame()\n",
    "\n",
    "for chunk in np.array_split(df_temp_2, chunks):\n",
    "    chunk = np.round(KNN(k=k).fit_transform(chunk))\n",
    "    df_chunk = pd.DataFrame(data=chunk, columns=cols)\n",
    "    df_impute = pd.concat([df_impute, df_chunk], axis=0, ignore_index=True)\n",
    "\n",
    "df_impute.to_csv('df_impute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell if you skipped the previous one.\n",
    "df_impute = pd.read_csv('df_impute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filling and splitting\n",
    "\n",
    "age_imputed = df_impute['age'].to_numpy()\n",
    "fa_imputed = df_impute['first_affiliate_tracked'].to_numpy()\n",
    "\n",
    "df_temp['age'] = age_imputed\n",
    "df_temp['first_affiliate_tracked'] = fa_imputed\n",
    "\n",
    "df_target = df_temp.xs(0)['country_destination']\n",
    "target = df_target.values\n",
    "\n",
    "df_temp.drop(['country_destination'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encoding Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = ['gender', 'signup_method', 'signup_flow', 'language',  'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "ct = ColumnTransformer([('encoder', OneHotEncoder(), categories)], remainder='passthrough')\n",
    "matrix = ct.fit_transform(df_temp)\n",
    "df = pd.DataFrame(matrix)\n",
    "\n",
    "train = df[:len(df_train)].values\n",
    "test = df[len(df_train):].values\n",
    "df_target = df_target.astype(int)\n",
    "target = df_target.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a multi-class classification problem. The prediction output for each row is the top 5 country destinations\n",
    "# with highest probability. Using sklearn accuracy metric won't give real accuracy.\n",
    "# NDCG (Normalized Discounted Cumulative Gain) is the evaluation metric used for this competition.\n",
    "\n",
    "def acc_ndcg(y_test, y_pred_proba):\n",
    "    acc = []\n",
    "    for i in range(len(y_pred_proba)):\n",
    "        p = np.argsort(y_pred_proba[i])[-5:][::-1]\n",
    "        t = y_test[i]\n",
    "        try:\n",
    "            index = int(np.where(p == t)[0][0]+1)\n",
    "            dcg = 1/(log2(index + 1))\n",
    "        except:\n",
    "            dcg = 0.0\n",
    "        acc.append(dcg)\n",
    "    return np.mean(acc)\n",
    "\n",
    "\n",
    "def clf_name(clf):\n",
    "    return clf.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# You can skip this cell. I've already trained and exported all the models.\n",
    "# Extensive grid search has been done to tune the parameters. Details in the file named: 'predict_extended.ipynb'\n",
    "# I've commented the section for SVC because it takes hours to fit. Comment out if you want to run it.\n",
    "# The models can be downloaded from here: https://drive.google.com/drive/folders/1CAMD8mVNjHKWsLxO5LIpswukDK_Vxtvy?usp=sharing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.33, random_state=28, shuffle=True)\n",
    "\n",
    "pd.DataFrame(X_train).to_csv('X_train.csv')\n",
    "pd.DataFrame(X_test).to_csv('X_test.csv')\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv')\n",
    "pd.DataFrame(y_test).to_csv('y_test.csv')\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "clf_cb = CatBoostClassifier(loss_function='MultiClass', task_type='GPU', max_depth=11, iterations=1000, learning_rate=0.01, random_state=28)\n",
    "classifiers.append(clf_cb)\n",
    "\n",
    "clf_xgb = XGBClassifier(max_depth=5, learning_rate=0.3, n_estimators=25, objective='multi:softprob', colsample_bytree=0.6, seed=28)\n",
    "classifiers.append(clf_xgb)\n",
    "\n",
    "clf_dtree = DecisionTreeClassifier(max_depth=4)\n",
    "classifiers.append(clf_dtree)\n",
    "\n",
    "clf_rndforrest = RandomForestClassifier(n_estimators=250, criterion='entropy', max_depth=11, min_samples_split=50, min_samples_leaf=50)\n",
    "classifiers.append(clf_rndforrest)\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=475, weights='distance')\n",
    "classifiers.append(clf_knn)\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    if clf_name(clf) == 'CatBoostClassifier':\n",
    "        clf.save_model(clf_name(clf)+'.json', format='json')\n",
    "    elif clf_name(clf) == 'XGBClassifier':\n",
    "        clf.save_model(clf_name(clf)+'.json')\n",
    "    else:\n",
    "        joblib.dump(clf, str(clf_name(clf)+'.joblib')\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_proba = clf.predict_proba(X_test)\n",
    "    acc = acc_ndcg(y_test, y_pred_proba)\n",
    "    print('Accuracy of %s: %s'%(clf_name(clf), acc))\n",
    "\n",
    "#clf_svm = SVC(decision_function_shape='ovo', probability=True, random_state=28) # Problem with name (RandomForrestClassifier) + Takes hours because one-vs-one\n",
    "#clf_svm.fit(X_train, y_train)\n",
    "#joblib.dump(clf_svm, 'SVM.joblib')\n",
    "#y_pred = clf_svm.predict(X_test)\n",
    "#y_pred_proba = clf_svm.predict_proba(X_test)\n",
    "#acc = acc_ndcg(y_test, y_pred_proba)\n",
    "#print('Accuracy of SVM: %s'%(acc)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy of CatBoostClassifier: 0.8168109083326747\nAccuracy of XGBClassifier: 0.8164678875307924\nAccuracy of DecisionTreeClassifier: 0.8137073989324712\nAccuracy of RandomForestClassifier: 0.8156357928545922\nAccuracy of KNeighborsClassifier: 0.7834471777253587\nAccuracy of SVC: 0.8151659500755046\n"
    }
   ],
   "source": [
    "# Run this cell if you skipped the previous one.\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.33, random_state=28, shuffle=True)\n",
    "\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_train.drop(X_train.columns[0], axis=1, inplace=True)\n",
    "X_train = X_train.values\n",
    "\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "X_test.drop(X_test.columns[0], axis=1, inplace=True)\n",
    "X_test = X_test.values\n",
    "\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_train.drop(y_train.columns[0], axis=1, inplace=True)\n",
    "y_train = y_train.values\n",
    "\n",
    "y_test = pd.read_csv('y_test.csv')\n",
    "y_test.drop(y_test.columns[0], axis=1, inplace=True)\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "clf_cb = CatBoostClassifier()\n",
    "clf_cb.load_model(clf_name(clf_cb)+'.json', 'json')\n",
    "y_pred_cb = clf_cb.predict(X_test)\n",
    "y_pred_proba_cb = clf_cb.predict_proba(X_test)\n",
    "print('Accuracy of %s: %s'%(clf_name(clf_cb), acc_ndcg(y_test, y_pred_proba_cb)))\n",
    "\n",
    "clf_xgb = XGBClassifier()\n",
    "clf_xgb.load_model(clf_name(clf_xgb)+'.json')\n",
    "y_pred_xgb = clf_xgb.predict(X_test)\n",
    "y_pred_proba_xgb = clf_xgb.predict_proba(X_test)\n",
    "print('Accuracy of %s: %s'%(clf_name(clf_xgb), acc_ndcg(y_test, y_pred_proba_xgb)))\n",
    "\n",
    "clf_dtree = DecisionTreeClassifier()\n",
    "clf_dtree = joblib.load(clf_name(clf_dtree)+'.joblib')\n",
    "y_pred_dtree = clf_dtree.predict(X_test)\n",
    "y_pred_proba_dtree = clf_dtree.predict_proba(X_test)\n",
    "print('Accuracy of %s: %s'%(clf_name(clf_dtree), acc_ndcg(y_test, y_pred_proba_dtree)))\n",
    "\n",
    "clf_rndforrest = RandomForestClassifier()\n",
    "clf_rndforrest = joblib.load(clf_name(clf_rndforrest)+'.joblib')\n",
    "y_pred_rndforrest = clf_rndforrest.predict(X_test)\n",
    "y_pred_proba_rndforrest = clf_rndforrest.predict_proba(X_test)\n",
    "print('Accuracy of %s: %s'%(clf_name(clf_rndforrest), acc_ndcg(y_test, y_pred_proba_rndforrest)))\n",
    "\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf_knn = joblib.load(clf_name(clf_knn)+'.joblib')\n",
    "y_pred_knn = clf_knn.predict(X_test)\n",
    "y_pred_proba_knn = clf_knn.predict_proba(X_test)\n",
    "print('Accuracy of %s: %s'%(clf_name(clf_knn), acc_ndcg(y_test, y_pred_proba_knn)))\n",
    "\n",
    "clf_svm = SVC()\n",
    "clf_svm = joblib.load('SVC.joblib')\n",
    "y_pred_svm = clf_svm.predict(X_test)\n",
    "y_pred_proba_svm = clf_svm.predict_proba(X_test)\n",
    "print('Accuracy of SVC: %s'%(acc_ndcg(y_test, y_pred_proba_svm)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predicting\n",
    "\n",
    "y_pred_proba = clf_cb.predict_proba(test)\n",
    "predictions = pd.DataFrame(columns=['id', 'country_destination'])\n",
    "temp = pd.read_csv('test_users.csv')\n",
    "temp = temp['id'].values\n",
    "idx = []\n",
    "cd = []\n",
    "for i in temp:\n",
    "    for _ in range(5):\n",
    "        idx.append(i)\n",
    "idx = np.array(idx)\n",
    "predictions['id'] = idx\n",
    "\n",
    "for i in range(len(y_pred_proba)):\n",
    "    pred = le.inverse_transform(np.argsort(y_pred_proba[i])[-5:][::-1])\n",
    "    for p in pred:\n",
    "        cd.append(p)\n",
    "cd = np.array(cd)\n",
    "predictions['country_destination'] = cd\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           id country_destination\n0  5uwns89zht                 NDF\n1  5uwns89zht                  US\n2  5uwns89zht               other\n3  5uwns89zht                  FR\n4  5uwns89zht                  IT",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>country_destination</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5uwns89zht</td>\n      <td>NDF</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5uwns89zht</td>\n      <td>US</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5uwns89zht</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5uwns89zht</td>\n      <td>FR</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5uwns89zht</td>\n      <td>IT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ]
}